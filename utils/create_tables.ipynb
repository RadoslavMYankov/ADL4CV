{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-05T20:04:17.904788Z",
     "start_time": "2025-02-05T20:04:17.901754Z"
    }
   },
   "source": [
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ],
   "outputs": [],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T20:04:17.950530Z",
     "start_time": "2025-02-05T20:04:17.947015Z"
    }
   },
   "cell_type": "code",
   "source": [
    "scene_name = \"bicycle\"\n",
    "local = False\n",
    "alt = \"\"\n",
    "local_text = \"_local\" if local else \"\"\n",
    "\n",
    "scenes = [\"berlin\", \"nyc\", \"london_v5\", \"alameda_v11\"]\n",
    "#scenes = [\"bicycle\", \"bonsai\", \"counter\", \"garden\", \"stump\"]\n",
    "\n",
    "metrics_to_process = [\"psnr\", \"ssim\", \"lpips\"]\n",
    "best_is_max = {\"psnr\": True, \"ssim\": True, \"lpips\": False}"
   ],
   "id": "69f824cc90b962cd",
   "outputs": [],
   "execution_count": 135
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T20:04:18.007112Z",
     "start_time": "2025-02-05T20:04:18.002272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "baseline_dict = {\n",
    "    \"EVER\": {\n",
    "        \"berlin\": [27.24, 0.900, 0.371],\n",
    "        \"nyc\": [27.93, 0.863, 0.337],\n",
    "\"alameda\": [24.72, 0.779, 0.389],\n",
    "\"london\": [26.49, 0.837, 0.374],\n",
    "\n",
    "    \"bicycle\": [25.34, 0.776, 0.220],\n",
    "        \"garden\": [27.46, 0.869, 0.120],\n",
    "        \"stump\": [26.41, 0.781, 0.230],\n",
    "        \"counter\": [28.91, 0.910, 0.24],\n",
    "        \"bonsai\": [32.24, 0.943, 0.236]\n",
    "    },\n",
    "    \"GSplat-7k\": {\n",
    "        \"bicycle\": [23.71, 0.662, 0.324],\n",
    "        \"garden\": [26.30, 0.833, 0.123],\n",
    "        \"stump\": [25.62, 0.720, 0.253],\n",
    "        \"counter\": [27.14, 0.878, 0.206],\n",
    "        \"bonsai\": [29.66, 0.922, 0.162]\n",
    "    },\n",
    "    \"3DGS MCMC\": {\n",
    "        \"counter\": [29.51, 0.92, 0.22],\n",
    "        \"stump\": [27.80, 0.82, 0.19],\n",
    "        \"bicycle\": [26.15, 0.81, 0.18],\n",
    "        \"bonsai\": [32.88, 0.95, 0.22],\n",
    "        \"garden\": [28.16, 0.89, 0.10]\n",
    "    }\n",
    "}"
   ],
   "id": "a0cfc6e9b693b9e",
   "outputs": [],
   "execution_count": 136
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T20:04:18.061313Z",
     "start_time": "2025-02-05T20:04:18.054922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "global_training_times = {\n",
    "    \"bicycle\": {\n",
    "    100: 3.0,\n",
    "    500: 14.2,\n",
    "    1000: 27.9,\n",
    "    2000: 55.6,\n",
    "    10000: 259.5\n",
    "},\n",
    "    \"counter\": {\n",
    "    100: 5.6,\n",
    "    500: 16.9,\n",
    "    1000: 30.9,\n",
    "    2000: 59.3,\n",
    "    10000: 268.0\n",
    "},\n",
    "    \"berlin\": {\n",
    "    1000: 30.1,\n",
    "    2000: 58.1,\n",
    "    5000: 134.7,\n",
    "    10000: 260.3\n",
    "},\n",
    "    \"nyc\": {\n",
    "    1000: 31.49,\n",
    "    2000: 59.35,\n",
    "    5000: 135.27,\n",
    "    10000: 261.06\n",
    "},\n",
    "    \"london\": {\n",
    "    1000: 35.34\n",
    "},\n",
    "    \"alameda\": {\n",
    "    1000: 36.97\n",
    "},\n",
    "    \"bonsai\": {\n",
    "    100: 6.86,\n",
    "    500: 17.71,\n",
    "    1000: 32.13,\n",
    "    2000: 59.68,\n",
    "    10000: 264.24\n",
    "},\n",
    "    \"garden\": {\n",
    "    100: 4.19,\n",
    "    500: 15.36,\n",
    "    1000: 29.06,\n",
    "    2000: 56.59,\n",
    "    10000: 258.29\n",
    "},\n",
    "    \"stump\": {\n",
    "    100: 3.36,\n",
    "    500: 14.24,\n",
    "    1000: 28.19,\n",
    "    2000: 56.14,\n",
    "    10000: 259.71\n",
    "}\n",
    "}\n",
    "local_training_times_by_scene = {\n",
    "    \"berlin\": {\n",
    "    200: 5.78 + 5.57 + 5.68 + 5.58 + 5.63,\n",
    "    1000: 27.79 + 27.87 + 27.89 + 27.87 + 27.69,\n",
    "    2000: 55.60 + 55.16 + 56.08 + 56.00 + 55.47,\n",
    "    5000: 132.34 + 131.59 + 130.79 + 132.46 + 131.65,\n",
    "    10000: 255.00 + 254.43 + 253.70 + 254.41 + 254.09,\n",
    "    \"1000_merged\": 29.09\n",
    "},\n",
    "    \"nyc\": {\n",
    "    200: 5.89 + 5.67 + 6.00 + 5.83 + 6.06,\n",
    "    1000: 28.24 + 27.60 + 27.89 + 27.91 + 27.26,\n",
    "    2000: 57.28 + 55.26 + 55.74 + 54.85 + 55.43,\n",
    "    \"1000_merged\": 30.11\n",
    "},\n",
    "    \"london\": {\n",
    "    \"1000_merged\": 30.14\n",
    "},\n",
    "    \"alameda\": {\n",
    "        \"1000_merged\": 30.30\n",
    "    }\n",
    "}"
   ],
   "id": "6e94018a89072c",
   "outputs": [],
   "execution_count": 137
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T20:04:18.116161Z",
     "start_time": "2025-02-05T20:04:18.113359Z"
    }
   },
   "cell_type": "code",
   "source": [
    "training_times_by_global_nerf_iterations = None\n",
    "training_times_by_local_nerf_iterations = None\n",
    "training_times = dict()\n"
   ],
   "id": "44360ccc1c990d4c",
   "outputs": [],
   "execution_count": 138
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T20:04:18.437306Z",
     "start_time": "2025-02-05T20:04:18.210784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_scene_results(scene, baseline_experiment=\"SfM\"):\n",
    "    data_by_metric = {}\n",
    "    name_map = {}\n",
    "    training_times = {}\n",
    "\n",
    "    for metric in metrics_to_process:\n",
    "        log_dir = f\"../tensorboard_results/{scene}{alt}{local_text}/{metric}\"\n",
    "        dfs = []\n",
    "        for root, dirs, files in os.walk(log_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    # For berlin, skip files that contain \"100k\"\n",
    "                    if \"processed_splatfacto_sparse_pc_30000_its\" not in file:\n",
    "                        if scene != \"stump\":\n",
    "                            if scene == \"berlin\" and \"100k\" in file:\n",
    "                                continue\n",
    "                            #elif scene == \"bicycle\" and ((\"1000\" not in file) or \"sparse\" in file):\n",
    "                            #    continue\n",
    "                            #elif (\"sparse_pc\" not in file) and (((\"150000\" not in file) and (\"150k\" not in file))):\n",
    "                            #    continue\n",
    "                            #elif (\"200\" in file) or (\"500its\" in file) or (\"100its\" in file) or (\"10000its\" in file) or (\"add\" in file):\n",
    "                            #    continue\n",
    "                        elif \"nerf_10000iters_init_30000_its\" in file or \"nerf_1000iters_init_30000_its\" in file:\n",
    "                            continue\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(full_path)\n",
    "\n",
    "                    experiment_name = os.path.splitext(file)[0]\n",
    "                    experiment_name = experiment_name.split(\"splatfacto_\")[-1]\n",
    "                    experiment_name = (experiment_name.replace(\"1mil\", \"1000000\")\n",
    "                                                   .replace(\"1m\", \"1000000\")\n",
    "                                                   .replace(\"1k\", \"1000\")\n",
    "                                                   .replace(\"0k\", \"0000\")\n",
    "                                                   .replace(\"add_\", \"\"))\n",
    "                    df['experiment'] = experiment_name\n",
    "                    dfs.append(df)\n",
    "\n",
    "                    # Build the mapping and record training time if not already set.\n",
    "                    if experiment_name not in name_map:\n",
    "                        if (\"merged_sfm\" in experiment_name) or (\"sfm_global\" in experiment_name):\n",
    "                            pattern = r'(\\d+)its_(\\d+)pts_(\\d+)_its'\n",
    "                            match = re.search(pattern, experiment_name)\n",
    "                            if match:\n",
    "                                nerf_iterations = int(match.group(1))\n",
    "                                points = int(match.group(2))\n",
    "                                display_name = f\"Global NeRF ({nerf_iterations}its) + SfM\"\n",
    "                                name_map[experiment_name] = display_name\n",
    "                                training_times[display_name] = global_training_times.get(scene.split(\"_\")[0], {}).get(nerf_iterations, 0.0)\n",
    "                            else:\n",
    "                                pattern = r'(\\d+)its_(\\d+)_'\n",
    "                                match = re.search(pattern, experiment_name)\n",
    "                                if match:\n",
    "                                    nerf_iterations = int(match.group(1))\n",
    "                                    display_name = f\"Global NeRF ({nerf_iterations}its) + SfM\"\n",
    "                                    name_map[experiment_name] = display_name\n",
    "                                    training_times[display_name] = global_training_times.get(scene.split(\"_\")[0], {}).get(nerf_iterations, 0.0)\n",
    "                                else:\n",
    "                                    name_map[experiment_name] = experiment_name\n",
    "                        elif \"sparse_pc\" in experiment_name:\n",
    "                            name_map[experiment_name] = \"SfM\"\n",
    "                        elif ((\"local_merged\" in experiment_name) or (\"merged_local\" in experiment_name)) and (\"1000\" in experiment_name):\n",
    "                            name_map[experiment_name] = \"Local NeRFs+ SfM\"\n",
    "                            training_times[name_map[experiment_name]] = local_training_times_by_scene.get(scene.split(\"_\")[0], {}).get(\"1000_merged\", 0.0)\n",
    "                        elif \"local_nerf\" in experiment_name:\n",
    "                            pattern = r'(\\d+)its_(\\d+)'\n",
    "                            match = re.search(pattern, experiment_name)\n",
    "                            if match:\n",
    "                                nerf_iterations = int(match.group(1))\n",
    "                                points = int(match.group(2))\n",
    "                                display_name = f\"Local NeRFs ({nerf_iterations}its, {points} pts) + SfM\"\n",
    "                                name_map[experiment_name] = display_name\n",
    "                                training_times[display_name] = local_training_times_by_scene.get(scene.split(\"_\")[0], {}).get(nerf_iterations, 0.0)\n",
    "                            else:\n",
    "                                name_map[experiment_name] = experiment_name\n",
    "                        elif \"global_nerf\" in experiment_name:\n",
    "                            pattern = r'(\\d+)its_(\\d+)_'\n",
    "                            match = re.search(pattern, experiment_name)\n",
    "                            if match:\n",
    "                                nerf_iterations = int(match.group(1))\n",
    "                                points = int(match.group(2))\n",
    "                                display_name = f\"Global NeRF ({nerf_iterations}its, {points} pts)\"\n",
    "                                name_map[experiment_name] = display_name\n",
    "                                training_times[display_name] = global_training_times.get(scene.split(\"_\")[0], {}).get(nerf_iterations, 0.0)\n",
    "                            else:\n",
    "                                name_map[experiment_name] = experiment_name\n",
    "                        elif \"nerf\" in experiment_name:\n",
    "                            pattern = r'(\\d+)_iters'\n",
    "                            match = re.search(pattern, experiment_name)\n",
    "                            if match:\n",
    "                                nerf_iterations = int(match.group(1))\n",
    "                                display_name = f\"Global NeRF ({nerf_iterations}its) only\"\n",
    "                                name_map[experiment_name] = display_name\n",
    "                                training_times[display_name] = global_training_times.get(scene.split(\"_\")[0], {}).get(nerf_iterations, 0.0)\n",
    "                            else:\n",
    "                                name_map[experiment_name] = experiment_name\n",
    "                        else:\n",
    "                            print(\"DROPPING\", experiment_name)\n",
    "                            continue\n",
    "                            name_map[experiment_name] = experiment_name\n",
    "        if dfs:\n",
    "            data_by_metric[metric] = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    def compute_best_within_baseline(df, experiments, baseline_experiment, best_is_max, by=\"time\"):\n",
    "        # Use only experiments that are in our mapping.\n",
    "        df = df[df['experiment'].isin(experiments.keys())].copy()\n",
    "        df['experiment_display'] = df['experiment'].map(experiments)\n",
    "        if \"Time\" not in df.columns and \"Wall time\" in df.columns:\n",
    "            df = df.rename(columns={\"Wall time\": \"Time\"})\n",
    "        x = \"Time\" if by==\"time\" else \"Step\"\n",
    "        baseline_df = df[df['experiment_display'].str.lower() == baseline_experiment.lower()]\n",
    "        if baseline_df.empty:\n",
    "            baseline_duration = float('inf')\n",
    "        else:\n",
    "            baseline_duration = baseline_df[x].max() - baseline_df[x].min()\n",
    "        best_dict = {}\n",
    "        for exp, group in df.groupby('experiment_display'):\n",
    "            if by==\"time\":\n",
    "                startup = training_times.get(exp, 0.0)\n",
    "                if startup == 0.0 and exp != baseline_experiment:\n",
    "                    print(f\"Warning: no training time found for {exp}, {scene_name}\")\n",
    "                group[x] = group[x] - group[x].min() + startup\n",
    "            group_within = group[group[x] <= baseline_duration]\n",
    "            if group_within.empty:\n",
    "                best_val = None\n",
    "            else:\n",
    "                best_val = group_within['Value'].max() if best_is_max else group_within['Value'].min()\n",
    "            best_dict[exp] = best_val\n",
    "        return best_dict\n",
    "\n",
    "    # Compute best values for each metric.\n",
    "    best_results = {}\n",
    "    for metric in metrics_to_process:\n",
    "        if metric not in data_by_metric:\n",
    "            continue\n",
    "        best_vals = compute_best_within_baseline(\n",
    "            data_by_metric[metric],\n",
    "            name_map,\n",
    "            baseline_experiment,\n",
    "            best_is_max[metric],\n",
    "            by=\"time\"\n",
    "        )\n",
    "        best_results[metric] = best_vals\n",
    "\n",
    "    # Merge best values into a single results dictionary.\n",
    "    scene_results = {}\n",
    "    for metric in metrics_to_process:\n",
    "        if metric in best_results:\n",
    "            for exp, val in best_results[metric].items():\n",
    "                if exp not in scene_results:\n",
    "                    scene_results[exp] = {}\n",
    "                col_name = f\"Best {metric.upper()}\"\n",
    "                scene_results[exp][col_name] = val\n",
    "\n",
    "    results_df = pd.DataFrame.from_dict(scene_results, orient='index').reset_index().rename(columns={\"index\": \"Experiment\"})\n",
    "    return results_df\n",
    "\n",
    "results_per_scene = {}\n",
    "for scene in scenes:\n",
    "    df_scene = compute_scene_results(scene, baseline_experiment=\"SfM\")\n",
    "    # Use Experiment as index, then rename metric columns to include scene name.\n",
    "    df_scene = df_scene.set_index(\"Experiment\")\n",
    "    rename_dict = {}\n",
    "    for col in df_scene.columns:\n",
    "        if col.startswith(\"Best \"):\n",
    "            metric = col.split(\" \")[1]\n",
    "            rename_dict[col] = f\"{scene}_{metric}\"\n",
    "    df_scene = df_scene.rename(columns=rename_dict)\n",
    "    results_per_scene[scene] = df_scene\n",
    "\n",
    "merged_results = None\n",
    "for scene, df in results_per_scene.items():\n",
    "    if merged_results is None:\n",
    "        merged_results = df.copy()\n",
    "    else:\n",
    "        merged_results = merged_results.join(df, how=\"outer\")\n",
    "merged_results = merged_results.reset_index()\n",
    "\n",
    "final_results = merged_results.copy()\n",
    "# Only keep experiments that appear in all scenes\n",
    "\n",
    "baseline_rows = []\n",
    "for baseline_name, scene_values in baseline_dict.items():\n",
    "    row = {\"Experiment\": baseline_name}\n",
    "    for col in final_results.columns:\n",
    "        if col == \"Experiment\":\n",
    "            continue\n",
    "        # Expecting columns like \"berlin_PSNR\"\n",
    "        parts = col.split(\"_\")\n",
    "        scene = parts[0]\n",
    "        metric = parts[-1]\n",
    "        if scene in scene_values:\n",
    "            if metric.upper() == \"PSNR\":\n",
    "                row[col] = scene_values[scene][0]\n",
    "            elif metric.upper() == \"SSIM\":\n",
    "                row[col] = scene_values[scene][1]\n",
    "            elif metric.upper() == \"LPIPS\":\n",
    "                row[col] = scene_values[scene][2]\n",
    "            else:\n",
    "                row[col] = np.nan\n",
    "        else:\n",
    "            row[col] = np.nan\n",
    "    baseline_rows.append(row)\n",
    "\n",
    "# Concatenate the baseline rows with final_results.\n",
    "final_results = pd.concat([final_results, pd.DataFrame(baseline_rows)], ignore_index=True)\n",
    "\n",
    "overall_best = {}\n",
    "best_experiment = {}\n",
    "for col in final_results.columns:\n",
    "    if col == \"Experiment\":\n",
    "        continue\n",
    "    metric = col.split(\"_\")[-1].upper()\n",
    "    if metric in [\"PSNR\", \"SSIM\"]:\n",
    "        overall_best[col] = final_results[col].max()\n",
    "        best_experiment[col] = final_results.loc[~final_results[\"Experiment\"].isin(baseline_dict.keys()), col].max()\n",
    "    elif metric == \"LPIPS\":\n",
    "        overall_best[col] = final_results[col].min()\n",
    "        best_experiment[col] = final_results.loc[~final_results[\"Experiment\"].isin(baseline_dict.keys()), col].min()\n",
    "\n",
    "def format_cell(value, col, row_type):\n",
    "    try:\n",
    "        num_val = float(value)\n",
    "    except:\n",
    "        return \"\"\n",
    "    s = f\"{num_val:.3f}\"\n",
    "    # Underline & bold if this is an experiment row and equals the best among experiments.\n",
    "    if row_type == \"experiment\" and np.isclose(num_val, best_experiment[col], atol=1e-6):\n",
    "        s = f\"\\\\underline{{\\\\textbf{{{num_val:.3f}}}}}\"\n",
    "    # Highlight if equals the overall best.\n",
    "    if np.isclose(num_val, overall_best[col], atol=1e-6):\n",
    "        s = f\"\\\\cellcolor{{red}}{s}\"\n",
    "    return s\n",
    "\n",
    "formatted_final = final_results.copy()\n",
    "for idx, row in final_results.iterrows():\n",
    "    row_type = \"baseline\" if row[\"Experiment\"] in baseline_dict.keys() else \"experiment\"\n",
    "    for col in final_results.columns:\n",
    "        if col == \"Experiment\":\n",
    "            continue\n",
    "        formatted_final.at[idx, col] = format_cell(row[col], col, row_type)\n",
    "\n",
    "best_is_max = {\"psnr\": True, \"ssim\": True, \"lpips\": False}\n",
    "new_columns = [(\"Experiment\", \"\")]\n",
    "for col in formatted_final.columns:\n",
    "    if col == \"Experiment\":\n",
    "        continue\n",
    "    parts = col.split(\"_\")\n",
    "    scene, metric = parts[0], parts[-1]\n",
    "    arrow = \" $\\\\uparrow$\" if best_is_max.get(metric.lower(), False) else \" $\\\\downarrow$\"\n",
    "    new_columns.append((scene.capitalize(), metric.upper() + arrow))\n",
    "multi_index = pd.MultiIndex.from_tuples(new_columns)\n",
    "formatted_final.columns = multi_index\n",
    "\n",
    "all_cols = list(formatted_final.columns)\n",
    "other_cols = all_cols[1:]  # all except \"Experiment\"\n",
    "n_other = len(other_cols)\n",
    "split_idx = math.ceil(n_other / 2)\n",
    "cols_part1 = [all_cols[0]] + other_cols[:split_idx]\n",
    "cols_part2 = [all_cols[0]] + other_cols[split_idx:]\n",
    "df_part1 = formatted_final[cols_part1]\n",
    "df_part2 = formatted_final[cols_part2]\n",
    "\n",
    "latex_table_part1 = df_part1.to_latex(escape=False, index=False, multicolumn=True, multicolumn_format='c')\n",
    "latex_table_part2 = df_part2.to_latex(escape=False, index=False, multicolumn=True, multicolumn_format='c')\n",
    "\n",
    "print(\"LaTeX Table Part 1:\")\n",
    "print(latex_table_part1)\n",
    "print(\"\\nLaTeX Table Part 2:\")\n",
    "print(latex_table_part2)"
   ],
   "id": "bab2c6e4b01b0977",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX Table Part 1:\n",
      "\\begin{tabular}{lllllll}\n",
      "\\toprule\n",
      "Experiment & \\multicolumn{3}{c}{Berlin} & \\multicolumn{3}{c}{Nyc} \\\\\n",
      " & PSNR $\\uparrow$ & SSIM $\\uparrow$ & LPIPS $\\downarrow$ & PSNR $\\uparrow$ & SSIM $\\uparrow$ & LPIPS $\\downarrow$ \\\\\n",
      "\\midrule\n",
      "Global NeRF (1000its) + SfM & 26.251 & 0.886 & 0.223 & 26.450 & 0.827 & 0.334 \\\\\n",
      "Local NeRFs+ SfM & \\underline{\\textbf{26.354}} & 0.887 & 0.221 & 26.557 & 0.828 & 0.332 \\\\\n",
      "SfM & 26.204 & \\underline{\\textbf{0.888}} & \\cellcolor{red}\\underline{\\textbf{0.217}} & \\underline{\\textbf{26.711}} & \\underline{\\textbf{0.830}} & \\cellcolor{red}\\underline{\\textbf{0.327}} \\\\\n",
      "EVER & \\cellcolor{red}27.240 & \\cellcolor{red}0.900 & 0.371 & \\cellcolor{red}27.930 & \\cellcolor{red}0.863 & 0.337 \\\\\n",
      "GSplat-7k & nan & nan & nan & nan & nan & nan \\\\\n",
      "3DGS MCMC & nan & nan & nan & nan & nan & nan \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "LaTeX Table Part 2:\n",
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "Experiment & \\multicolumn{2}{c}{London} & \\multicolumn{3}{c}{Alameda} \\\\\n",
      " & PSNR $\\uparrow$ & LPIPS $\\downarrow$ & PSNR $\\uparrow$ & SSIM $\\uparrow$ & LPIPS $\\downarrow$ \\\\\n",
      "\\midrule\n",
      "Global NeRF (1000its) + SfM & 25.346 & \\underline{\\textbf{0.379}} & 21.730 & 0.707 & 0.468 \\\\\n",
      "Local NeRFs+ SfM & \\underline{\\textbf{25.379}} & 0.384 & \\underline{\\textbf{21.802}} & \\underline{\\textbf{0.708}} & \\underline{\\textbf{0.467}} \\\\\n",
      "SfM & 25.327 & 0.385 & 21.588 & 0.705 & 0.471 \\\\\n",
      "EVER & \\cellcolor{red}26.490 & \\cellcolor{red}0.374 & \\cellcolor{red}24.720 & \\cellcolor{red}0.779 & \\cellcolor{red}0.389 \\\\\n",
      "GSplat-7k & nan & nan & nan & nan & nan \\\\\n",
      "3DGS MCMC & nan & nan & nan & nan & nan \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_225123/2143060490.py:240: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '26.251' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  formatted_final.at[idx, col] = format_cell(row[col], col, row_type)\n",
      "/tmp/ipykernel_225123/2143060490.py:240: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.886' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  formatted_final.at[idx, col] = format_cell(row[col], col, row_type)\n",
      "/tmp/ipykernel_225123/2143060490.py:240: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.223' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  formatted_final.at[idx, col] = format_cell(row[col], col, row_type)\n",
      "/tmp/ipykernel_225123/2143060490.py:240: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '26.450' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  formatted_final.at[idx, col] = format_cell(row[col], col, row_type)\n",
      "/tmp/ipykernel_225123/2143060490.py:240: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.827' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  formatted_final.at[idx, col] = format_cell(row[col], col, row_type)\n",
      "/tmp/ipykernel_225123/2143060490.py:240: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.334' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  formatted_final.at[idx, col] = format_cell(row[col], col, row_type)\n",
      "/tmp/ipykernel_225123/2143060490.py:240: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '25.346' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  formatted_final.at[idx, col] = format_cell(row[col], col, row_type)\n",
      "/tmp/ipykernel_225123/2143060490.py:240: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '\\underline{\\textbf{0.379}}' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  formatted_final.at[idx, col] = format_cell(row[col], col, row_type)\n",
      "/tmp/ipykernel_225123/2143060490.py:240: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '21.730' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  formatted_final.at[idx, col] = format_cell(row[col], col, row_type)\n",
      "/tmp/ipykernel_225123/2143060490.py:240: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.707' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  formatted_final.at[idx, col] = format_cell(row[col], col, row_type)\n",
      "/tmp/ipykernel_225123/2143060490.py:240: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '0.468' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  formatted_final.at[idx, col] = format_cell(row[col], col, row_type)\n"
     ]
    }
   ],
   "execution_count": 139
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-05T20:04:18.683060Z",
     "start_time": "2025-02-05T20:04:18.504357Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_scene_time_to_reach(scene, baseline_experiment=\"SfM\"):\n",
    "    data_by_metric = {}\n",
    "    name_map = {}\n",
    "    training_times = {}\n",
    "\n",
    "    for metric in metrics_to_process:\n",
    "        log_dir = f\"../tensorboard_results/{scene}{alt}{local_text}/{metric}\"\n",
    "        dfs = []\n",
    "        for root, dirs, files in os.walk(log_dir):\n",
    "            for file in files:\n",
    "                if file.endswith(\".csv\"):\n",
    "                    # For berlin, skip files that contain \"100k\"\n",
    "                    if \"processed_splatfacto_sparse_pc_30000_its\" not in file:\n",
    "                        if scene != \"stump\":\n",
    "                            if scene == \"berlin\" and \"100k\" in file:\n",
    "                                continue\n",
    "                            #elif scene == \"bicycle\" and ((\"1000\" not in file) or \"sparse\" in file):\n",
    "                            #    continue\n",
    "                            #elif (\"sparse_pc\" not in file) and (((\"150000\" not in file) and (\"150k\" not in file))):\n",
    "                            #    continue\n",
    "                            #elif (\"200\" in file) or (\"500its\" in file) or (\"100its\" in file) or (\"10000its\" in file) or (\"add\" in file):\n",
    "                            #    continue\n",
    "                        elif \"nerf_10000iters_init_30000_its\" in file or \"nerf_1000iters_init_30000_its\" in file:\n",
    "                            continue\n",
    "                    full_path = os.path.join(root, file)\n",
    "                    df = pd.read_csv(full_path)\n",
    "\n",
    "                    experiment_name = os.path.splitext(file)[0]\n",
    "                    experiment_name = experiment_name.split(\"splatfacto_\")[-1]\n",
    "                    experiment_name = (experiment_name.replace(\"1mil\", \"1000000\")\n",
    "                                                   .replace(\"1m\", \"1000000\")\n",
    "                                                   .replace(\"1k\", \"1000\")\n",
    "                                                   .replace(\"0k\", \"0000\")\n",
    "                                                   .replace(\"add_\", \"\"))\n",
    "                    df['experiment'] = experiment_name\n",
    "                    dfs.append(df)\n",
    "\n",
    "                    # Build the mapping and record training time if not already set.\n",
    "                    if experiment_name not in name_map:\n",
    "                        if (\"merged_sfm\" in experiment_name) or (\"sfm_global\" in experiment_name):\n",
    "                            pattern = r'(\\d+)its_(\\d+)pts_(\\d+)_its'\n",
    "                            match = re.search(pattern, experiment_name)\n",
    "                            if match:\n",
    "                                nerf_iterations = int(match.group(1))\n",
    "                                points = int(match.group(2))\n",
    "                                display_name = f\"Global NeRF ({nerf_iterations}its) + SfM\"\n",
    "                                name_map[experiment_name] = display_name\n",
    "                                training_times[display_name] = global_training_times.get(scene.split(\"_\")[0], {}).get(nerf_iterations, 0.0)\n",
    "                            else:\n",
    "                                pattern = r'(\\d+)its_(\\d+)_'\n",
    "                                match = re.search(pattern, experiment_name)\n",
    "                                if match:\n",
    "                                    nerf_iterations = int(match.group(1))\n",
    "                                    display_name = f\"Global NeRF ({nerf_iterations}its) + SfM\"\n",
    "                                    name_map[experiment_name] = display_name\n",
    "                                    training_times[display_name] = global_training_times.get(scene.split(\"_\")[0], {}).get(nerf_iterations, 0.0)\n",
    "                                else:\n",
    "                                    name_map[experiment_name] = experiment_name\n",
    "                        elif \"sparse_pc\" in experiment_name:\n",
    "                            name_map[experiment_name] = \"SfM\"\n",
    "                        elif ((\"local_merged\" in experiment_name) or (\"merged_local\" in experiment_name)) and (\"1000\" in experiment_name):\n",
    "                            name_map[experiment_name] = \"Local NeRFs+ SfM\"\n",
    "                            training_times[name_map[experiment_name]] = local_training_times_by_scene.get(scene.split(\"_\")[0], {}).get(\"1000_merged\", 0.0)\n",
    "                        elif \"local_nerf\" in experiment_name:\n",
    "                            pattern = r'(\\d+)its_(\\d+)'\n",
    "                            match = re.search(pattern, experiment_name)\n",
    "                            if match:\n",
    "                                nerf_iterations = int(match.group(1))\n",
    "                                points = int(match.group(2))\n",
    "                                display_name = f\"Local NeRFs ({nerf_iterations}its, {points} pts) + SfM\"\n",
    "                                name_map[experiment_name] = display_name\n",
    "                                training_times[display_name] = local_training_times_by_scene.get(scene.split(\"_\")[0], {}).get(nerf_iterations, 0.0)\n",
    "                            else:\n",
    "                                name_map[experiment_name] = experiment_name\n",
    "                        elif \"global_nerf\" in experiment_name:\n",
    "                            pattern = r'(\\d+)its_(\\d+)_'\n",
    "                            match = re.search(pattern, experiment_name)\n",
    "                            if match:\n",
    "                                nerf_iterations = int(match.group(1))\n",
    "                                points = int(match.group(2))\n",
    "                                display_name = f\"Global NeRF ({nerf_iterations}its, {points} pts)\"\n",
    "                                name_map[experiment_name] = display_name\n",
    "                                training_times[display_name] = global_training_times.get(scene.split(\"_\")[0], {}).get(nerf_iterations, 0.0)\n",
    "                            else:\n",
    "                                name_map[experiment_name] = experiment_name\n",
    "                        elif \"nerf\" in experiment_name:\n",
    "                            pattern = r'(\\d+)_iters'\n",
    "                            match = re.search(pattern, experiment_name)\n",
    "                            if match:\n",
    "                                nerf_iterations = int(match.group(1))\n",
    "                                display_name = f\"Global NeRF ({nerf_iterations}its) only\"\n",
    "                                name_map[experiment_name] = display_name\n",
    "                                training_times[display_name] = global_training_times.get(scene.split(\"_\")[0], {}).get(nerf_iterations, 0.0)\n",
    "                            else:\n",
    "                                name_map[experiment_name] = experiment_name\n",
    "                        else:\n",
    "                            print(\"DROPPING\", experiment_name)\n",
    "                            continue\n",
    "                            name_map[experiment_name] = experiment_name\n",
    "        if dfs:\n",
    "            data_by_metric[metric] = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    def compute_time_to_reach(df, experiments, baseline_experiment, best_is_max, by=\"time\"):\n",
    "        df = df[df['experiment'].isin(experiments.keys())].copy()\n",
    "        df['experiment_display'] = df['experiment'].map(experiments)\n",
    "        if \"Time\" not in df.columns and \"Wall time\" in df.columns:\n",
    "            df = df.rename(columns={\"Wall time\": \"Time\"})\n",
    "        x = \"Time\" if by==\"time\" else \"Step\"\n",
    "        # Get the baseline data for the baseline_experiment.\n",
    "        baseline_df = df[df['experiment_display'].str.lower() == baseline_experiment.lower()]\n",
    "        if baseline_df.empty:\n",
    "            threshold = None\n",
    "            baseline_duration = float('inf')\n",
    "        else:\n",
    "            if best_is_max:\n",
    "                threshold = baseline_df['Value'].max()\n",
    "            else:\n",
    "                threshold = baseline_df['Value'].min()\n",
    "            baseline_duration = baseline_df[x].max() - baseline_df[x].min()\n",
    "        time_dict = {}\n",
    "        for exp, group in df.groupby('experiment_display'):\n",
    "            if by==\"time\":\n",
    "                startup = training_times.get(exp, 0.0)\n",
    "                group = group.copy()\n",
    "                group[x] = group[x] - group[x].min() + startup\n",
    "            # Find the first time at which the experiment reaches the threshold.\n",
    "            if threshold is None:\n",
    "                time_dict[exp] = None\n",
    "                continue\n",
    "            if best_is_max:\n",
    "                reached = group[group['Value'] >= threshold]\n",
    "            else:\n",
    "                reached = group[group['Value'] <= threshold]\n",
    "            if reached.empty:\n",
    "                time_dict[exp] = None\n",
    "            else:\n",
    "                time_dict[exp] = reached[x].iloc[0]\n",
    "        return time_dict\n",
    "\n",
    "    time_to_reach = {}\n",
    "    for metric in metrics_to_process:\n",
    "        if metric not in data_by_metric:\n",
    "            continue\n",
    "        times = compute_time_to_reach(data_by_metric[metric], name_map, baseline_experiment, best_is_max[metric], by=\"time\")\n",
    "        time_to_reach[metric] = times\n",
    "\n",
    "    scene_time_results = {}\n",
    "    for metric in metrics_to_process:\n",
    "        if metric not in time_to_reach:\n",
    "            continue\n",
    "        col_name = f\"Time {metric.upper()}\"\n",
    "        for exp, t in time_to_reach[metric].items():\n",
    "            if exp not in scene_time_results:\n",
    "                scene_time_results[exp] = {}\n",
    "            scene_time_results[exp][col_name] = t\n",
    "    results_df = pd.DataFrame.from_dict(scene_time_results, orient='index').reset_index().rename(columns={\"index\": \"Experiment\"})\n",
    "    return results_df\n",
    "\n",
    "results_time_per_scene = {}\n",
    "for scene in scenes:\n",
    "    df_scene_time = compute_scene_time_to_reach(scene, baseline_experiment=\"SfM\")\n",
    "    df_scene_time = df_scene_time.set_index(\"Experiment\")\n",
    "    rename_dict = {}\n",
    "    for col in df_scene_time.columns:\n",
    "        metric = col.split(\" \")[1]\n",
    "        rename_dict[col] = f\"{scene}_{metric}\"\n",
    "    df_scene_time = df_scene_time.rename(columns=rename_dict)\n",
    "    results_time_per_scene[scene] = df_scene_time\n",
    "\n",
    "merged_time_results = None\n",
    "for scene, df in results_time_per_scene.items():\n",
    "    if merged_time_results is None:\n",
    "        merged_time_results = df.copy()\n",
    "    else:\n",
    "        merged_time_results = merged_time_results.join(df, how=\"outer\")\n",
    "merged_time_results = merged_time_results.reset_index()\n",
    "\n",
    "#print(\"Time-to-reach baseline table:\")\n",
    "#print(merged_time_results)\n",
    "\n",
    "def compute_speedup_table(merged_df, baseline_experiment=\"SfM\"):\n",
    "    df = merged_df.copy()\n",
    "    for col in df.columns:\n",
    "        if col == \"Experiment\":\n",
    "            continue\n",
    "        baseline_val = df.loc[df[\"Experiment\"] == baseline_experiment, col]\n",
    "        if baseline_val.empty:\n",
    "            continue\n",
    "        baseline_val = baseline_val.iloc[0]\n",
    "        def calc_speedup(x):\n",
    "            try:\n",
    "                x_val = float(x)\n",
    "                if x_val == 0:\n",
    "                    return \"N/A\"\n",
    "                return baseline_val / x_val\n",
    "            except:\n",
    "                return \"N/A\"\n",
    "        df[col] = df[col].apply(calc_speedup)\n",
    "    return df\n",
    "\n",
    "speedup_df = compute_speedup_table(merged_time_results, baseline_experiment=\"SfM\")\n",
    "\n",
    "def format_speedup(value):\n",
    "    try:\n",
    "        v = float(value)\n",
    "        return f\"{v:.2f}x\"\n",
    "    except:\n",
    "        return \"N/A\"\n",
    "\n",
    "for col in speedup_df.columns:\n",
    "    if col == \"Experiment\":\n",
    "        continue\n",
    "    speedup_df[col] = speedup_df[col].apply(lambda x: float(x) if x != \"N/A\" else np.nan)\n",
    "\n",
    "for col in speedup_df.columns:\n",
    "    if col == \"Experiment\":\n",
    "        continue\n",
    "    max_val = speedup_df[col].max(skipna=True)\n",
    "    def fmt_cell(x):\n",
    "        if pd.isna(x):\n",
    "            return \"N/A\"\n",
    "        s = f\"{x:.2f}x\"\n",
    "        if np.isclose(x, max_val, atol=1e-6):\n",
    "            # Highlight best speedup.\n",
    "            return f\"\\\\cellcolor{{red}}{{{s}}}\"\n",
    "        return s\n",
    "    speedup_df[col] = speedup_df[col].apply(fmt_cell)\n",
    "\n",
    "new_columns = [(\"Experiment\", \"\")]\n",
    "for col in speedup_df.columns:\n",
    "    if col == \"Experiment\":\n",
    "        continue\n",
    "    parts = col.split(\"_\")\n",
    "    scene, metric = parts[0], parts[-1]\n",
    "    arrow = r\" $\\uparrow$\" if best_is_max.get(metric.lower(), False) else r\" $\\downarrow$\"\n",
    "    new_columns.append((scene.capitalize(), metric.upper() + arrow))\n",
    "speedup_df.columns = pd.MultiIndex.from_tuples(new_columns)\n",
    "\n",
    "all_cols = list(speedup_df.columns)\n",
    "other_cols = all_cols[1:]\n",
    "split_idx = math.ceil(len(other_cols)/2)\n",
    "cols_part1 = [all_cols[0]] + other_cols[:split_idx]\n",
    "cols_part2 = [all_cols[0]] + other_cols[split_idx:]\n",
    "df_speedup_part1 = speedup_df[cols_part1]\n",
    "df_speedup_part2 = speedup_df[cols_part2]\n",
    "\n",
    "latex_speedup_part1 = df_speedup_part1.to_latex(escape=False, index=False, multicolumn=True, multicolumn_format='c')\n",
    "latex_speedup_part2 = df_speedup_part2.to_latex(escape=False, index=False, multicolumn=True, multicolumn_format='c')\n",
    "\n",
    "print(\"\\nLaTeX Speedup Table Part 1:\")\n",
    "print(latex_speedup_part1)\n",
    "print(\"\\nLaTeX Speedup Table Part 2:\")\n",
    "print(latex_speedup_part2)"
   ],
   "id": "8abc4f708aeb6267",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LaTeX Speedup Table Part 1:\n",
      "\\begin{tabular}{lllllll}\n",
      "\\toprule\n",
      "Experiment & \\multicolumn{3}{c}{Berlin} & \\multicolumn{3}{c}{Nyc} \\\\\n",
      " & PSNR $\\uparrow$ & SSIM $\\uparrow$ & LPIPS $\\downarrow$ & PSNR $\\uparrow$ & SSIM $\\uparrow$ & LPIPS $\\downarrow$ \\\\\n",
      "\\midrule\n",
      "Global NeRF (1000its) + SfM & 1.00x & 0.92x & 0.91x & N/A & 0.87x & 0.90x \\\\\n",
      "Local NeRFs+ SfM & \\cellcolor{red}{1.02x} & 0.96x & 0.94x & 0.89x & 0.94x & 0.93x \\\\\n",
      "SfM & 1.00x & \\cellcolor{red}{1.00x} & \\cellcolor{red}{1.00x} & \\cellcolor{red}{1.00x} & \\cellcolor{red}{1.00x} & \\cellcolor{red}{1.00x} \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n",
      "\n",
      "LaTeX Speedup Table Part 2:\n",
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "Experiment & \\multicolumn{2}{c}{London} & \\multicolumn{3}{c}{Alameda} \\\\\n",
      " & PSNR $\\uparrow$ & LPIPS $\\downarrow$ & PSNR $\\uparrow$ & SSIM $\\uparrow$ & LPIPS $\\downarrow$ \\\\\n",
      "\\midrule\n",
      "Global NeRF (1000its) + SfM & 0.99x & \\cellcolor{red}{1.07x} & 1.05x & 1.04x & 1.02x \\\\\n",
      "Local NeRFs+ SfM & \\cellcolor{red}{1.01x} & 1.00x & \\cellcolor{red}{1.07x} & \\cellcolor{red}{1.06x} & \\cellcolor{red}{1.04x} \\\\\n",
      "SfM & 1.00x & 1.00x & 1.00x & 1.00x & 1.00x \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "execution_count": 140
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
